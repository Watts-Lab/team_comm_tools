{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_builder import ModelBuilder\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'becker': {'filename': 'beckerestimation_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'mean_pre_discussion_error',\n",
       "   'mean_post_discussion_error',\n",
       "   'mean_pre_discussion_error_pct',\n",
       "   'mean_post_discussion_error_pct',\n",
       "   'question',\n",
       "   'chatrooms',\n",
       "   'trial_indx']},\n",
       " 'csop': {'filename': 'csop_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'batch_num',\n",
       "   'round_num',\n",
       "   'round_index',\n",
       "   'task_index',\n",
       "   'complexity',\n",
       "   'type',\n",
       "   'social_perceptiveness',\n",
       "   'skill',\n",
       "   'normalized_score',\n",
       "   'zscore_score',\n",
       "   'zscore_round_duration',\n",
       "   'zscore_efficiency']},\n",
       " 'csopII': {'filename': 'csopII_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'batch_num',\n",
       "   'vis_img',\n",
       "   'int_verb',\n",
       "   'ort_img',\n",
       "   'rep_man',\n",
       "   'soc_pers',\n",
       "   'team_size',\n",
       "   'difficulty',\n",
       "   'score',\n",
       "   'duration',\n",
       "   'efficiency',\n",
       "   'timestamp']},\n",
       " 'dat': {'filename': 'DAT_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'batch_num',\n",
       "   'vis_img',\n",
       "   'int_verb',\n",
       "   'ort_img',\n",
       "   'rep_man',\n",
       "   'soc_pers',\n",
       "   'team_size',\n",
       "   'score',\n",
       "   'duration',\n",
       "   'efficiency',\n",
       "   'timestamp']},\n",
       " 'gurcay': {'filename': 'gurcay2015estimation_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'mean_pre_discussion_error',\n",
       "   'mean_post_discussion_error',\n",
       "   'mean_pre_discussion_error_pct',\n",
       "   'mean_post_discussion_error_pct',\n",
       "   'question']},\n",
       " 'juries': {'filename': 'jury_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'batch_num',\n",
       "   'round_num',\n",
       "   'timestamp',\n",
       "   'majority_pct',\n",
       "   'num_flipped',\n",
       "   'flipped_pct',\n",
       "   'num_votes']},\n",
       " 'pgg': {'filename': 'pgg_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'treatmentName',\n",
       "   'punishmentCost',\n",
       "   'punishmentMagnitude',\n",
       "   'total_costs',\n",
       "   'total_penalties',\n",
       "   'total_rewards',\n",
       "   'multiplier',\n",
       "   'punishmentExists',\n",
       "   'rewardExists',\n",
       "   'playerCount',\n",
       "   'numRounds',\n",
       "   'timestamp',\n",
       "   'score']},\n",
       " 'task_mapping_keys': {'juries': 'Mock jury',\n",
       "  'csop': 'Room assignment task',\n",
       "  'csopII': 'Room assignment task',\n",
       "  'dat': 'Divergent Association Task',\n",
       "  'becker': 'Estimating Factual Quantities',\n",
       "  'gurcay': 'Estimating Factual Quantities'},\n",
       " 'task_names': ['Mock jury',\n",
       "  'Room assignment task',\n",
       "  'Room assignment task',\n",
       "  'Divergent Association Task',\n",
       "  'Estimating Factual Quantities']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config.json\", \"rb\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated k-Fold Cross Validation\n",
    "For each model, fit it k (=10) times, and track all metrics, as well as interpretability, across all the repeats.\n",
    "This gets us a less noisy estimate at our models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_kfold_cv(model, k = 10, seed = 19104):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - model: The model we are doing k-fold CV for\n",
    "    - k: the number of fols (defaults to 10)\n",
    "    - seed: the random seed (defaults to 19104)\n",
    "\n",
    "    @returns the following, pouplated with data from the k0=-fold CV:\n",
    "    - train_metrics: a dataframe to store all the training metrics\n",
    "    - test_metrics: a dataframe to store all the test set metrics (we will universally use a 80-20 train-test split)\n",
    "    - shap_df: a dataframe to store the Shapley value summaries for each fold\n",
    "    - shap_correlation_df: a dataframe to store how the Shapley values correlate with feature values for each fold\n",
    "    \"\"\"\n",
    "\n",
    "    # Repeated k-fold cross-validation\n",
    "    random.seed(seed) # set seed for reproducibility\n",
    "    random_states_list = [random.randint(100, 1000000) for _ in range(k)] # create a bunch of different random states\n",
    "\n",
    "    # Store metrics --- R^2, MAE, MSE\n",
    "    metrics = ['r2', 'mae', 'mse', 'rmse']\n",
    "    train_metrics = pd.DataFrame(columns=metrics)\n",
    "    test_metrics = pd.DataFrame(columns=metrics)\n",
    "\n",
    "    for i in range(len(random_states_list)):\n",
    "        # store the model metrics for each iteration\n",
    "        metrics = model.evaluate_model(model.baseline_model, val_size = 0.2, test_size = None, random_state = random_states_list[i], visualize_model = False)\n",
    "        train_metrics = train_metrics.append(metrics['train'], ignore_index=True)\n",
    "        test_metrics = test_metrics.append(metrics['val'], ignore_index=True)\n",
    "    \n",
    "        # store the shap summary for each iteration\n",
    "\n",
    "        try:     \n",
    "            shap_summary = model.shap_summary\n",
    "            shap_df = pd.merge(shap_df, shap_summary[['feature', 'shap']], on='feature')\n",
    "            shap_df.rename(columns={'shap': f'shap_{i+1}'}, inplace=True)\n",
    "            shap_correlation_df = pd.merge(shap_correlation_df, shap_summary[['feature', 'correlation_btw_shap_and_feature_value']], on='feature')\n",
    "            shap_correlation_df.rename(columns={'correlation_btw_shap_and_feature_value': f'cor_{i+1}'}, inplace=True)\n",
    "        except NameError:\n",
    "            # we haven't defined these yet; we're in the first iteration!\n",
    "            # we have to do this becaus model.X does not show up until after the first case when evaluate_model is called\n",
    "            shap_df = pd.DataFrame({'feature': model.X.columns})\n",
    "            shap_correlation_df = pd.DataFrame({'feature': model.X.columns})\n",
    "\n",
    "            shap_summary = model.shap_summary\n",
    "            shap_df = pd.merge(shap_df, shap_summary[['feature', 'shap']], on='feature')\n",
    "            shap_df.rename(columns={'shap': f'shap_{i+1}'}, inplace=True)\n",
    "            shap_correlation_df = pd.merge(shap_correlation_df, shap_summary[['feature', 'correlation_btw_shap_and_feature_value']], on='feature')\n",
    "            shap_correlation_df.rename(columns={'correlation_btw_shap_and_feature_value': f'cor_{i+1}'}, inplace=True)\n",
    "\n",
    "\n",
    "    shap_df.set_index('feature', inplace=True)\n",
    "    shap_correlation_df.set_index('feature', inplace=True)\n",
    "\n",
    "    return(shap_df, shap_correlation_df, train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repeated_kfold_cv_summary(shap_df, shap_correlation_df, train_metrics, test_metrics):\n",
    "    \"\"\"\n",
    "    Get the means of the repeated k-fold cross validation across all relevant metrics.\n",
    "    \"\"\"\n",
    "    shap_means = shap_df.mean(axis=1).sort_values(ascending = False)\n",
    "    shap_cor_means = shap_correlation_df.mean(axis=1).reindex(index = shap_means.index)\n",
    "    train_means = train_metrics.mean()\n",
    "    test_means = test_metrics.mean()\n",
    "\n",
    "    return(shap_means, shap_cor_means, train_means, test_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points = [\"25%\", \"50%\", \"75%\", \"100%\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOP_FEATURES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_features_over_time(merged_df, color_palette, title=\"Top Feature Importance Over Time\", filename=\"./figures/feature_importance.png\"):\n",
    "    # Transpose the DataFrame and sort by each time point\n",
    "    top_features = merged_df.apply(lambda x: x.nlargest(NUM_TOP_FEATURES))\n",
    "\n",
    "    non_na_feature = top_features.dropna()\n",
    "    na_feature = top_features.loc[~top_features.index.isin(top_features.dropna().index)].fillna(0)\n",
    "\n",
    "    # Plot a line chart to show how the top feature values change over time\n",
    "    if not non_na_feature.empty:\n",
    "        ax = non_na_feature.T.plot(kind='line', marker='o', linewidth=3, color=color_palette)\n",
    "        na_feature.T.plot(kind='line', marker='o', linestyle='--', linewidth=3, ax=ax, color=color_palette)  # Use the same axis for dashed lines\n",
    "    else:\n",
    "        ax = na_feature.T.plot(kind='line', marker='o', linestyle='--', linewidth=3, color=color_palette)\n",
    "\n",
    "    plt.ylabel('Importance (SHAP value)', size=14)\n",
    "    plt.xlabel('Percent of Chat Messages (Chronological)', size=14)\n",
    "    plt.title(title, fontsize=18, fontweight=\"bold\")\n",
    "    plt.xticks(range(len(time_points)), time_points, fontsize=14)\n",
    "\n",
    "    # Update legend with custom color mapping\n",
    "    legend_labels = ax.get_legend().get_texts()\n",
    "    for label in legend_labels:\n",
    "        feature_name = label.get_text()\n",
    "\n",
    "    plt.legend(loc='upper left', fontsize=12, bbox_to_anchor=(1.05, 1), bbox_transform=ax.transAxes)\n",
    "\n",
    "    plt.savefig(filename, dpi=1200, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_and_mse_over_time(metrics, title):\n",
    "    # Transpose the data for plotting\n",
    "    transposed_data = metrics.T\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    plt.style.use({\"figure.facecolor\": \"white\", \"axes.facecolor\": \"white\"})\n",
    "\n",
    "    # Create the left y-axis for R^2\n",
    "    ax1.set_ylabel(\"R^2\", fontsize=14)\n",
    "    ax1.plot(time_points, transposed_data[\"r2\"], label=\"R^2\", color=\"cadetblue\", marker=\"o\", linewidth=3)\n",
    "    ax1.tick_params(axis=\"y\", size=14)\n",
    "\n",
    "    # Create the right y-axis for MSE\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel(\"MSE\", fontsize=14)\n",
    "    ax2.plot(time_points, transposed_data[\"mse\"], label=\"MSE\", color=\"mediumorchid\", marker=\"o\", linewidth=3)\n",
    "    ax2.tick_params(axis=\"y\", size=14)\n",
    "\n",
    "    #x-axis font size\n",
    "    ax1.tick_params(axis=\"x\", labelsize=14)\n",
    "    ax1.set_xlabel('Percent of Chat Messages (Chronological)', size=14)\n",
    "\n",
    "    # Combine the legends for both lines\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc=\"upper left\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.title(title, fontweight=\"bold\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Task Baseline Models\n",
    "This notebook contains the _baseline models_ for each task. This allows us to get a quick understanding of the predictive features for each task (separately).\n",
    "\n",
    "The lists of models are:\n",
    "\n",
    "(1) Random Forests for each task:\n",
    "- Jury\n",
    "- CSOP Blended (across two datasets)\n",
    "- CSOP Train -> CSOP II split\n",
    "- DAT\n",
    "- Estimation (Becker + Gurcay datasets, blended)\n",
    "\n",
    "(2) Early Cut-off Models (Train model only on the first X% of the messages, so that we avoid issues where the final stages of the discussion reveal the outcome)\n",
    "- 25% (all datasets)\n",
    "- 50% (all datasets)\n",
    "- 75% (all datasets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jury"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Data (100)%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the basic model\n",
    "juries_model = ModelBuilder(dataset_names = [\"juries\"])\n",
    "juries_model.select_target(target=[\"majority_pct\"])\n",
    "juries_model.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Cleaning Up Columns...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Cleaning Up Columns...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Cleaning Up Columns...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Cleaning Up Columns...\n"
     ]
    }
   ],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "jury_shap, jury_shap_cor, jury_train_metrics, jury_test_metrics = repeated_kfold_cv(juries_model)\n",
    "jury_shap_means, jury_shap_cor_means, jury_train_means, jury_test_means = get_repeated_kfold_cv_summary(jury_shap, jury_shap_cor, jury_train_metrics, jury_test_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juries_model75 = ModelBuilder(dataset_names = [\"juries\"], output_dir = '../output/first_75/')\n",
    "juries_model75.select_target(target=[\"majority_pct\"])\n",
    "juries_model75.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury_shap75, jury_shap_cor75, jury_train_metrics75, jury_test_metrics75 = repeated_kfold_cv(juries_model75)\n",
    "jury_shap_means75, jury_shap_cor_means75, jury_train_means75, jury_test_means75 = get_repeated_kfold_cv_summary(jury_shap75, jury_shap_cor75, jury_train_metrics75, jury_test_metrics75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juries_model50 = ModelBuilder(dataset_names = [\"juries\"], output_dir = '../output/first_50/')\n",
    "juries_model50.select_target(target=[\"majority_pct\"])\n",
    "juries_model50.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury_shap50, jury_shap_cor50, jury_train_metrics50, jury_test_metrics50 = repeated_kfold_cv(juries_model50)\n",
    "jury_shap_means50, jury_shap_cor_means50, jury_train_means50, jury_test_means50 = get_repeated_kfold_cv_summary(jury_shap50, jury_shap_cor50, jury_train_metrics50, jury_test_metrics50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25% of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juries_model25 = ModelBuilder(dataset_names = [\"juries\"], output_dir = '../output/first_25/')\n",
    "juries_model25.select_target(target=[\"majority_pct\"])\n",
    "juries_model25.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury_shap25, jury_shap_cor25, jury_train_metrics25, jury_test_metrics25 = repeated_kfold_cv(juries_model25)\n",
    "jury_shap_means25, jury_shap_cor_means25, jury_train_means25, jury_test_means25 = get_repeated_kfold_cv_summary(jury_shap25, jury_shap_cor25, jury_train_metrics25, jury_test_metrics25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Results for Juries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([jury_shap_cor_means25, jury_shap_cor_means50, jury_shap_cor_means75, jury_shap_cor_means], keys = time_points, axis=1).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Feature Color Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([jury_shap_means25, jury_shap_means50, jury_shap_means75, jury_shap_means], keys = time_points, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_COLORS = 40245*len(merged_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_color(feature_name):\n",
    "    hashed = hash(feature_name) % NUM_COLORS\n",
    "    hue = hashed / 1000.0\n",
    "    saturation = (hashed % NUM_COLORS) / NUM_COLORS  # Vary the saturation within a range (0.7 to 1.0)\n",
    "    value = 0.8\n",
    "    color_rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "    return color_rgb\n",
    "\n",
    "# Create a color mapping dictionary\n",
    "color_mapping = {feature_name: generate_color(feature_name) for feature_name in merged_df.index}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_important_features_over_time(merged_df, color_mapping, \"Mock Jury\", \"./figures/jury_features.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pd.concat([jury_test_means25, jury_test_means50, jury_test_means75, jury_test_means], keys = time_points, axis=1)\n",
    "plot_r2_and_mse_over_time(test_metrics, \"R^2 and MSE over time, Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at direction of impact\n",
    "pd.concat([jury_shap_cor_means25, jury_shap_cor_means50, jury_shap_cor_means75, jury_shap_cor_means], keys = time_points, axis=1).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSOP (blended)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Data (100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csop_blended_model = ModelBuilder(dataset_names = [\"csop\", \"csopII\"])\n",
    "csop_blended_model.select_target(target=[\"zscore_efficiency\", \"efficiency\"])\n",
    "csop_blended_model.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csop_shap, csop_shap_cor, csop_train_metrics, csop_test_metrics = repeated_kfold_cv(csop_blended_model)\n",
    "csop_shap_means, csop_shap_cor_means, csop_train_means, csop_test_means = get_repeated_kfold_cv_summary(csop_shap, csop_shap_cor, csop_train_metrics, csop_test_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csop_blended_model75 = ModelBuilder(dataset_names = [\"csop\", \"csopII\"], output_dir = '../output/first_75/')\n",
    "csop_blended_model75.select_target(target=[\"zscore_efficiency\", \"efficiency\"])\n",
    "csop_blended_model75.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csop_shap75, csop_shap_cor75, csop_train_metrics75, csop_test_metrics75 = repeated_kfold_cv(csop_blended_model75)\n",
    "csop_shap_means75, csop_shap_cor_means75, csop_train_means75, csop_test_means75 = get_repeated_kfold_cv_summary(csop_shap75, csop_shap_cor75, csop_train_metrics75, csop_test_metrics75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csop_blended_model50 = ModelBuilder(dataset_names = [\"csop\", \"csopII\"], output_dir = '../output/first_50/')\n",
    "csop_blended_model50.select_target(target=[\"zscore_efficiency\", \"efficiency\"])\n",
    "csop_blended_model50.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csop_shap50, csop_shap_cor50, csop_train_metrics50, csop_test_metrics50 = repeated_kfold_cv(csop_blended_model50)\n",
    "csop_shap_means50, csop_shap_cor_means50, csop_train_means50, csop_test_means50 = get_repeated_kfold_cv_summary(csop_shap50, csop_shap_cor50, csop_train_metrics50, csop_test_metrics50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csop_blended_model25 = ModelBuilder(dataset_names = [\"csop\", \"csopII\"], output_dir = '../output/first_25/')\n",
    "csop_blended_model25.select_target(target=[\"zscore_efficiency\", \"efficiency\"])\n",
    "csop_blended_model25.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csop_shap25, csop_shap_cor25, csop_train_metrics25, csop_test_metrics25 = repeated_kfold_cv(csop_blended_model25)\n",
    "csop_shap_means25, csop_shap_cor_means25, csop_train_means25, csop_test_means25 = get_repeated_kfold_cv_summary(csop_shap25, csop_shap_cor25, csop_train_metrics25, csop_test_metrics25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine CSOP Blended Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csop_shap_means, csop_shap_cor_means, csop_train_means, csop_test_means\n",
    "merged_df = pd.concat([csop_shap_means25, csop_shap_means50, csop_shap_means75, csop_shap_means], keys = time_points, axis=1)\n",
    "plot_important_features_over_time(merged_df, color_mapping, \"Room Assigment Task\", \"./figures/csop_features.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pd.concat([csop_test_means25, csop_test_means50, csop_test_means75, csop_test_means], keys = time_points, axis=1)\n",
    "plot_r2_and_mse_over_time(test_metrics, \"R^2 and MSE over time, Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at direction of impact\n",
    "pd.concat([csop_shap_cor_means25, csop_shap_cor_means50, csop_shap_cor_means75, csop_shap_cor_means], keys = time_points, axis=1).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full data (100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_model = ModelBuilder(dataset_names = [\"dat\"])\n",
    "dat_model.select_target(target=[\"efficiency\"])\n",
    "dat_model.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_shap, dat_shap_cor, dat_train_metrics, dat_test_metrics = repeated_kfold_cv(dat_model)\n",
    "dat_shap_means, dat_shap_cor_means, dat_train_means, dat_test_means = get_repeated_kfold_cv_summary(dat_shap, dat_shap_cor, dat_train_metrics, dat_test_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_model75 = ModelBuilder(dataset_names = [\"dat\"], output_dir = '../output/first_75/')\n",
    "dat_model75.select_target(target=[\"efficiency\"])\n",
    "dat_model75.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_shap75, dat_shap_cor75, dat_train_metrics75, dat_test_metrics75 = repeated_kfold_cv(dat_model75)\n",
    "dat_shap_means75, dat_shap_cor_means75, dat_train_means75, dat_test_means75 = get_repeated_kfold_cv_summary(dat_shap75, dat_shap_cor75, dat_train_metrics75, dat_test_metrics75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_model50 = ModelBuilder(dataset_names = [\"dat\"], output_dir = '../output/first_50/')\n",
    "dat_model50.select_target(target=[\"efficiency\"])\n",
    "dat_model50.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_shap50, dat_shap_cor50, dat_train_metrics50, dat_test_metrics50 = repeated_kfold_cv(dat_model50)\n",
    "dat_shap_means50, dat_shap_cor_means50, dat_train_means50, dat_test_means50 = get_repeated_kfold_cv_summary(dat_shap50, dat_shap_cor50, dat_train_metrics50, dat_test_metrics50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_model25 = ModelBuilder(dataset_names = [\"dat\"], output_dir = '../output/first_25/')\n",
    "dat_model25.select_target(target=[\"efficiency\"])\n",
    "dat_model25.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_shap25, dat_shap_cor25, dat_train_metrics25, dat_test_metrics25 = repeated_kfold_cv(dat_model25)\n",
    "dat_shap_means25, dat_shap_cor_means25, dat_train_means25, dat_test_means25 = get_repeated_kfold_cv_summary(dat_shap25, dat_shap_cor25, dat_train_metrics25, dat_test_metrics25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine DAT Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([dat_shap_means25, dat_shap_means50, dat_shap_means75, dat_shap_means], keys = time_points, axis=1)\n",
    "plot_important_features_over_time(merged_df, color_mapping, \"Divergent Association Task\", \"./figures/dat_features.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pd.concat([dat_test_means25, dat_test_means50, dat_test_means75, dat_test_means], keys = time_points, axis=1)\n",
    "plot_r2_and_mse_over_time(test_metrics, \"R^2 and MSE over time, Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at direction of impact\n",
    "pd.concat([dat_shap_cor_means25, dat_shap_cor_means50, dat_shap_cor_means75, dat_shap_cor_means], keys = time_points, axis=1).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Data (100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_blended_model = ModelBuilder(dataset_names = [\"gurcay\", \"becker\"])\n",
    "estimation_blended_model.select_target(target=[\"mean_post_discussion_error_pct\", \"mean_post_discussion_error_pct\"])\n",
    "estimation_blended_model.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_shap, estimation_shap_cor, estimation_train_metrics, estimation_test_metrics = repeated_kfold_cv(estimation_blended_model)\n",
    "estimation_shap_means, estimation_shap_cor_means, estimation_train_means, estimation_test_means = get_repeated_kfold_cv_summary(estimation_shap, estimation_shap_cor, estimation_train_metrics, estimation_test_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_blended_model75 = ModelBuilder(dataset_names = [\"gurcay\", \"becker\"],  output_dir = '../output/first_75/')\n",
    "estimation_blended_model75.select_target(target=[\"mean_post_discussion_error_pct\", \"mean_post_discussion_error_pct\"])\n",
    "estimation_blended_model75.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_shap75, estimation_shap_cor75, estimation_train_metrics75, estimation_test_metrics75 = repeated_kfold_cv(estimation_blended_model75)\n",
    "estimation_shap_means75, estimation_shap_cor_means75, estimation_train_means75, estimation_test_means75 = get_repeated_kfold_cv_summary(estimation_shap75, estimation_shap_cor75, estimation_train_metrics75, estimation_test_metrics75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_blended_model50 = ModelBuilder(dataset_names = [\"gurcay\", \"becker\"],  output_dir = '../output/first_50/')\n",
    "estimation_blended_model50.select_target(target=[\"mean_post_discussion_error_pct\", \"mean_post_discussion_error_pct\"])\n",
    "estimation_blended_model50.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_shap50, estimation_shap_cor50, estimation_train_metrics50, estimation_test_metrics50 = repeated_kfold_cv(estimation_blended_model50)\n",
    "estimation_shap_means50, estimation_shap_cor_means50, estimation_train_means50, estimation_test_means50 = get_repeated_kfold_cv_summary(estimation_shap50, estimation_shap_cor50, estimation_train_metrics50, estimation_test_metrics50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_blended_model25 = ModelBuilder(dataset_names = [\"gurcay\", \"becker\"],  output_dir = '../output/first_25/')\n",
    "estimation_blended_model25.select_target(target=[\"mean_post_discussion_error_pct\", \"mean_post_discussion_error_pct\"])\n",
    "estimation_blended_model25.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_shap25, estimation_shap_cor25, estimation_train_metrics25, estimation_test_metrics25 = repeated_kfold_cv(estimation_blended_model25)\n",
    "estimation_shap_means25, estimation_shap_cor_means25, estimation_train_means25, estimation_test_means25 = get_repeated_kfold_cv_summary(estimation_shap25, estimation_shap_cor25, estimation_train_metrics25, estimation_test_metrics25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Estimation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([estimation_shap_means25, estimation_shap_means50, estimation_shap_means75, estimation_shap_means], keys = time_points, axis=1)\n",
    "plot_important_features_over_time(merged_df, color_mapping, \"Estimation\", \"./figures/estimation_features.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pd.concat([estimation_test_means25, estimation_test_means50, estimation_test_means75, estimation_test_means], keys = time_points, axis=1)\n",
    "plot_r2_and_mse_over_time(test_metrics, \"R^2 and MSE over time, Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at direction of impact\n",
    "pd.concat([dat_shap_cor_means25, dat_shap_cor_means50, dat_shap_cor_means75, dat_shap_cor_means], keys = time_points, axis=1).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Metrics into Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury_metrics = pd.concat([jury_test_means25, jury_test_means50, jury_test_means75, jury_test_means], keys = time_points, axis=1) \n",
    "csop_metrics = pd.concat([csop_test_means25, csop_test_means50, csop_test_means75, csop_test_means], keys = time_points, axis=1)\n",
    "dat_metrics = pd.concat([dat_test_means25, dat_test_means50, dat_test_means75, dat_test_means], keys = time_points, axis=1)\n",
    "estimation_metrics = pd.concat([estimation_test_means25, estimation_test_means50, estimation_test_means75, estimation_test_means], keys = time_points, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_with_broken_axis(datasets, dataset_names, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 6))\n",
    "\n",
    "    plt.style.use({\"figure.facecolor\": \"white\", \"axes.facecolor\": \"white\"})\n",
    "\n",
    "    # Define line styles and colors from Dark2 colormap\n",
    "    num_colors = len(dataset_names)\n",
    "    colors = mpl.cm.Dark2(np.linspace(0, 1, num_colors))\n",
    "\n",
    "    # Plot positive datasets on ax1\n",
    "    for i, dataset in enumerate([jury_metrics, csop_metrics]):\n",
    "        dataset_name = dataset_names[i]\n",
    "        transposed_data = dataset.T\n",
    "\n",
    "        # Create the left y-axis for R^2 on the first plot\n",
    "        ax1.plot(time_points, transposed_data[\"r2\"], marker='o', label=dataset_name, color=colors[i], linewidth=3)\n",
    "        for x, y in zip(time_points, transposed_data[\"r2\"]):\n",
    "            ax1.annotate(f'{y:.2f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "\n",
    "    # Set limits for ax1\n",
    "    ax1.set_ylim(0, 0.13)  # Adjust the limits as needed\n",
    "\n",
    "    # Plot negative datasets on ax2\n",
    "    for i, dataset in enumerate([dat_metrics, estimation_metrics]):\n",
    "        j = i+2 #MESSY -- but this is manually set to the negative datasets (DAT, Estimation), and assumes they are the last 2 in the list\n",
    "        dataset_name = dataset_names[j] \n",
    "        transposed_data = dataset.T\n",
    "\n",
    "        # Create the left y-axis for R^2 on the first plot\n",
    "        ax2.plot(time_points, transposed_data[\"r2\"], marker='o', label=dataset_name, color=colors[j], linewidth=3)\n",
    "        for x, y in zip(time_points, transposed_data[\"r2\"]):\n",
    "            ax2.annotate(f'{y:.2f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "\n",
    "    # Set limits for ax2\n",
    "    ax2.set_ylim(-28, 0.01)  # Adjust the limits as needed\n",
    "\n",
    "    # Remove the space between the two plots\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "\n",
    "    # Adjust layout and labels\n",
    "    ax1.tick_params(axis=\"both\", labelsize=16)  # Increase tick label size\n",
    "    ax2.tick_params(axis=\"both\", labelsize=16)  # Increase tick label size\n",
    "    ax1.set_ylabel(\"R^2\", fontsize=16)\n",
    "    ax2.set_ylabel(\"R^2\", fontsize=16)\n",
    "    ax2.set_xlabel(\"Percent of Chat Messages (Chronological)\", fontsize=16)\n",
    "\n",
    "    # Move the legend to the right\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    ax1.legend(lines, labels, loc=\"upper left\", fontsize=14, bbox_to_anchor=(1, 1.005))  # Adjust the y-coordinate\n",
    "    lines, labels = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines, labels, loc=\"upper left\", fontsize=14, bbox_to_anchor=(1, 1.005))\n",
    "\n",
    "    # Add broken y-axis symbol\n",
    "    d = .005  # Offset for markings\n",
    "    kwargs = dict(transform=ax1.transAxes, color='k', clip_on=False)\n",
    "    ax1.plot((1-d,1+d), (-d,+d), **kwargs)\n",
    "    ax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\n",
    "\n",
    "    # Adjust title position to the top\n",
    "    plt.suptitle(title, fontweight=\"bold\", fontsize=18)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig('./figures/validation_test_r2_graph.svg', dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "datasets = [jury_metrics, csop_metrics, dat_metrics, estimation_metrics]\n",
    "dataset_names = [\"Mock Jury\", \"Room Assignment Task\", \"Divergent Association Task\", \"Estimation\"]\n",
    "\n",
    "plot_r2_with_broken_axis(datasets, dataset_names, \"Validation R^2 of Models Over Time, Per Task\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jury_metrics = pd.concat([jury_train_means25, jury_train_means50, jury_train_means75, jury_train_means], keys = time_points, axis=1) \n",
    "csop_metrics = pd.concat([csop_train_means25, csop_train_means50, csop_train_means75, csop_train_means], keys = time_points, axis=1)\n",
    "dat_metrics = pd.concat([dat_train_means25, dat_train_means50, dat_train_means75, dat_train_means], keys = time_points, axis=1)\n",
    "estimation_metrics = pd.concat([estimation_train_means25, estimation_train_means50, estimation_train_means75, estimation_train_means], keys = time_points, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_without_broken_axis(datasets, dataset_names, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    plt.style.use({\"figure.facecolor\": \"white\", \"axes.facecolor\": \"white\"})\n",
    "\n",
    "    # Define line styles and colors from Dark2 colormap\n",
    "    num_colors = len(dataset_names)\n",
    "    colors = mpl.cm.Dark2(np.linspace(0, 1, num_colors))\n",
    "\n",
    "    # Plot datasets on the same y-axis\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        dataset_name = dataset_names[i]\n",
    "        transposed_data = dataset.T\n",
    "\n",
    "        # Plot R^2 values for each dataset\n",
    "        ax.plot(time_points, transposed_data[\"r2\"], marker='o', label=dataset_name, color=colors[i], linewidth=3)\n",
    "        for x, y in zip(time_points, transposed_data[\"r2\"]):\n",
    "            ax.annotate(f'{y:.2f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "\n",
    "    # Adjust layout and labels\n",
    "    ax.tick_params(axis=\"both\", labelsize=16)  # Increase tick label size\n",
    "    ax.set_ylabel(\"R^2\", fontsize=16)\n",
    "    ax.set_xlabel(\"Percent of Chat Messages (Chronological)\", fontsize=16)\n",
    "\n",
    "    # Add legend to the right\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(lines, labels, loc=\"upper left\", fontsize=14, bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Set y-axis range with a buffer\n",
    "    ax.set_ylim(min(ax.get_ylim()), max(ax.get_ylim())*1.01)\n",
    "\n",
    "    # Adjust title position\n",
    "    plt.title(title, fontweight=\"bold\", fontsize=18)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figures/train_r2_graph.svg', dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have defined the four datasets and their names as you described\n",
    "datasets = [jury_metrics, csop_metrics, dat_metrics, estimation_metrics]\n",
    "dataset_names = [\"Mock Jury\", \"Room Assignment Task\", \"Divergent Association Task\", \"Estimation\"]\n",
    "\n",
    "# Call the function to plot R^2 values without broken y-axis\n",
    "plot_r2_without_broken_axis(datasets, dataset_names, \"Training R^2 of Models Over Time, Per Task\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team_process_map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4308115ec36d55d4bd05e5164490d17bc30a5f7275b0a0d4f3922ff237a9eaea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
