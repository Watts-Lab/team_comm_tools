{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_builder import ModelBuilder\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'becker': {'filename': 'beckerestimation_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'mean_pre_discussion_error',\n",
       "   'mean_post_discussion_error',\n",
       "   'mean_pre_discussion_error_pct',\n",
       "   'mean_post_discussion_error_pct',\n",
       "   'question',\n",
       "   'chatrooms',\n",
       "   'trial_indx']},\n",
       " 'csop': {'filename': 'csop_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'batch_num',\n",
       "   'round_num',\n",
       "   'round_index',\n",
       "   'task_index',\n",
       "   'complexity',\n",
       "   'type',\n",
       "   'social_perceptiveness',\n",
       "   'skill',\n",
       "   'normalized_score',\n",
       "   'zscore_score',\n",
       "   'zscore_round_duration',\n",
       "   'zscore_efficiency']},\n",
       " 'csopII': {'filename': 'csopII_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'batch_num',\n",
       "   'vis_img',\n",
       "   'int_verb',\n",
       "   'ort_img',\n",
       "   'rep_man',\n",
       "   'soc_pers',\n",
       "   'team_size',\n",
       "   'difficulty',\n",
       "   'score',\n",
       "   'duration',\n",
       "   'efficiency',\n",
       "   'timestamp']},\n",
       " 'dat': {'filename': 'DAT_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'batch_num',\n",
       "   'vis_img',\n",
       "   'int_verb',\n",
       "   'ort_img',\n",
       "   'rep_man',\n",
       "   'soc_pers',\n",
       "   'team_size',\n",
       "   'score',\n",
       "   'duration',\n",
       "   'efficiency',\n",
       "   'timestamp']},\n",
       " 'gurcay': {'filename': 'gurcay2015estimation_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'mean_pre_discussion_error',\n",
       "   'mean_post_discussion_error',\n",
       "   'mean_pre_discussion_error_pct',\n",
       "   'mean_post_discussion_error_pct',\n",
       "   'question']},\n",
       " 'juries': {'filename': 'jury_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'batch_num',\n",
       "   'round_num',\n",
       "   'timestamp',\n",
       "   'majority_pct',\n",
       "   'num_flipped',\n",
       "   'flipped_pct',\n",
       "   'num_votes']},\n",
       " 'pgg': {'filename': 'pgg_output_conversation_level.csv',\n",
       "  'cols_to_ignore': ['conversation_num',\n",
       "   'treatmentName',\n",
       "   'punishmentCost',\n",
       "   'punishmentMagnitude',\n",
       "   'total_costs',\n",
       "   'total_penalties',\n",
       "   'total_rewards',\n",
       "   'multiplier',\n",
       "   'punishmentExists',\n",
       "   'rewardExists',\n",
       "   'playerCount',\n",
       "   'numRounds',\n",
       "   'timestamp',\n",
       "   'score']},\n",
       " 'task_mapping_keys': {'juries': 'Mock jury',\n",
       "  'csop': 'Room assignment task',\n",
       "  'csopII': 'Room assignment task',\n",
       "  'dat': 'Divergent Association Task',\n",
       "  'becker': 'Estimating Factual Quantities',\n",
       "  'gurcay': 'Estimating Factual Quantities'},\n",
       " 'task_names': ['Mock jury',\n",
       "  'Room assignment task',\n",
       "  'Room assignment task',\n",
       "  'Divergent Association Task',\n",
       "  'Estimating Factual Quantities']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config.json\", \"rb\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_kfold_cv(model, k = 10, seed = 19104):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - model: The model we are doing k-fold CV for\n",
    "    - k: the number of fols (defaults to 10)\n",
    "    - seed: the random seed (defaults to 19104)\n",
    "\n",
    "    @returns the following, pouplated with data from the k0=-fold CV:\n",
    "    - train_metrics: a dataframe to store all the training metrics\n",
    "    - test_metrics: a dataframe to store all the test set metrics (we will universally use a 80-20 train-test split)\n",
    "    - shap_df: a dataframe to store the Shapley value summaries for each fold\n",
    "    - shap_correlation_df: a dataframe to store how the Shapley values correlate with feature values for each fold\n",
    "    \"\"\"\n",
    "\n",
    "    # Repeated k-fold cross-validation\n",
    "    random.seed(seed) # set seed for reproducibility\n",
    "    random_states_list = [random.randint(100, 1000000) for _ in range(k)] # create a bunch of different random states\n",
    "\n",
    "    # Store metrics --- R^2, MAE, MSE\n",
    "    metrics = ['r2', 'mae', 'mse', 'rmse']\n",
    "    train_metrics = pd.DataFrame(columns=metrics)\n",
    "    test_metrics = pd.DataFrame(columns=metrics)\n",
    "\n",
    "    for i in range(len(random_states_list)):\n",
    "        # store the model metrics for each iteration\n",
    "        metrics = model.evaluate_model(model.baseline_model, val_size = 0.2, test_size = None, random_state = random_states_list[i], visualize_model = False)\n",
    "        train_metrics = train_metrics.append(metrics['train'], ignore_index=True)\n",
    "        test_metrics = test_metrics.append(metrics['val'], ignore_index=True)\n",
    "    \n",
    "        # store the shap summary for each iteration\n",
    "\n",
    "        try:     \n",
    "            shap_summary = model.shap_summary\n",
    "            shap_df = pd.merge(shap_df, shap_summary[['feature', 'shap']], on='feature')\n",
    "            shap_df.rename(columns={'shap': f'shap_{i+1}'}, inplace=True)\n",
    "            shap_correlation_df = pd.merge(shap_correlation_df, shap_summary[['feature', 'correlation_btw_shap_and_feature_value']], on='feature')\n",
    "            shap_correlation_df.rename(columns={'correlation_btw_shap_and_feature_value': f'cor_{i+1}'}, inplace=True)\n",
    "        except NameError:\n",
    "            # we haven't defined these yet; we're in the first iteration!\n",
    "            # we have to do this becaus model.X does not show up until after the first case when evaluate_model is called\n",
    "            shap_df = pd.DataFrame({'feature': model.X.columns})\n",
    "            shap_correlation_df = pd.DataFrame({'feature': model.X.columns})\n",
    "\n",
    "            shap_summary = model.shap_summary\n",
    "            shap_df = pd.merge(shap_df, shap_summary[['feature', 'shap']], on='feature')\n",
    "            shap_df.rename(columns={'shap': f'shap_{i+1}'}, inplace=True)\n",
    "            shap_correlation_df = pd.merge(shap_correlation_df, shap_summary[['feature', 'correlation_btw_shap_and_feature_value']], on='feature')\n",
    "            shap_correlation_df.rename(columns={'correlation_btw_shap_and_feature_value': f'cor_{i+1}'}, inplace=True)\n",
    "\n",
    "\n",
    "    shap_df.set_index('feature', inplace=True)\n",
    "    shap_correlation_df.set_index('feature', inplace=True)\n",
    "\n",
    "    return(shap_df, shap_correlation_df, train_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repeated_kfold_cv_summary(shap_df, shap_correlation_df, train_metrics, test_metrics):\n",
    "    \"\"\"\n",
    "    Get the means of the repeated k-fold cross validation across all relevant metrics.\n",
    "    \"\"\"\n",
    "    shap_means = shap_df.mean(axis=1).sort_values(ascending = False)\n",
    "    shap_cor_means = shap_correlation_df.mean(axis=1).reindex(index = shap_means.index)\n",
    "    train_means = train_metrics.mean()\n",
    "    test_means = test_metrics.mean()\n",
    "\n",
    "    return(shap_means, shap_cor_means, train_means, test_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points = [\"25%\", \"50%\", \"75%\", \"100%\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOP_FEATURES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_features_over_time(merged_df, color_palette, title=\"Top Feature Importance Over Time\", filename=\"./figures/feature_importance.png\"):\n",
    "    # Transpose the DataFrame and sort by each time point\n",
    "    top_features = merged_df.apply(lambda x: x.nlargest(NUM_TOP_FEATURES))\n",
    "\n",
    "    non_na_feature = top_features.dropna()\n",
    "    na_feature = top_features.loc[~top_features.index.isin(top_features.dropna().index)].fillna(0)\n",
    "\n",
    "    # Plot a line chart to show how the top feature values change over time\n",
    "    if not non_na_feature.empty:\n",
    "        ax = non_na_feature.T.plot(kind='line', marker='o', linewidth=3, color=color_palette)\n",
    "        na_feature.T.plot(kind='line', marker='o', linestyle='--', linewidth=3, ax=ax, color=color_palette)  # Use the same axis for dashed lines\n",
    "    else:\n",
    "        ax = na_feature.T.plot(kind='line', marker='o', linestyle='--', linewidth=3, color=color_palette)\n",
    "\n",
    "    plt.ylabel('Importance (SHAP value)', size=14)\n",
    "    plt.xlabel('Percent of Chat Messages (Chronological)', size=14)\n",
    "    plt.title(title, fontsize=18, fontweight=\"bold\")\n",
    "    plt.xticks(range(len(time_points)), time_points, fontsize=14)\n",
    "\n",
    "    # Update legend with custom color mapping\n",
    "    legend_labels = ax.get_legend().get_texts()\n",
    "    for label in legend_labels:\n",
    "        feature_name = label.get_text()\n",
    "\n",
    "    plt.legend(loc='upper left', fontsize=12, bbox_to_anchor=(1.05, 1), bbox_transform=ax.transAxes)\n",
    "\n",
    "    plt.savefig(filename, dpi=1200, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Joint Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model (standardizing features _across_ tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = ModelBuilder(dataset_names = [\"juries\", \"csop\", \"csopII\", \"becker\", \"dat\"])\n",
    "joint_model.select_target(target=[\"majority_pct\", \"zscore_efficiency\", \"efficiency\", \"mean_post_discussion_error_pct\", \"score\"])\n",
    "joint_model.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n"
     ]
    }
   ],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "joint_shap, joint_shap_cor, joint_train_metrics, joint_test_metrics = repeated_kfold_cv(joint_model)\n",
    "joint_shap_means, joint_shap_cor_means, joint_train_means, joint_test_means = get_repeated_kfold_cv_summary(joint_shap, joint_shap_cor, joint_train_metrics, joint_test_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model75 = ModelBuilder(dataset_names = [\"juries\", \"csop\", \"csopII\", \"becker\", \"dat\"], output_dir = '../output/first_75/')\n",
    "joint_model75.select_target(target=[\"majority_pct\", \"zscore_efficiency\", \"efficiency\", \"mean_post_discussion_error_pct\", \"score\"])\n",
    "joint_model75.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n"
     ]
    }
   ],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "joint_shap75, joint_shap_cor75, joint_train_metrics75, joint_test_metrics75 = repeated_kfold_cv(joint_model)\n",
    "joint_shap_means75, joint_shap_cor_means75, joint_train_means75, joint_test_means75 = get_repeated_kfold_cv_summary(joint_shap75, joint_shap_cor75, joint_train_metrics75, joint_test_metrics75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model50 = ModelBuilder(dataset_names = [\"juries\", \"csop\", \"csopII\", \"becker\", \"dat\"], output_dir = '../output/first_50/')\n",
    "joint_model50.select_target(target=[\"majority_pct\", \"zscore_efficiency\", \"efficiency\", \"mean_post_discussion_error_pct\", \"score\"])\n",
    "joint_model50.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n"
     ]
    }
   ],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "joint_shap50, joint_shap_cor50, joint_train_metrics50, joint_test_metrics50 = repeated_kfold_cv(joint_model50)\n",
    "joint_shap_means50, joint_shap_cor_means50, joint_train_means50, joint_test_means50 = get_repeated_kfold_cv_summary(joint_shap50, joint_shap_cor50, joint_train_metrics50, joint_test_metrics50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25% Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model25 = ModelBuilder(dataset_names = [\"juries\", \"csop\", \"csopII\", \"becker\", \"dat\"], output_dir = '../output/first_25/')\n",
    "joint_model25.select_target(target=[\"majority_pct\", \"zscore_efficiency\", \"efficiency\", \"mean_post_discussion_error_pct\", \"score\"])\n",
    "joint_model25.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n",
      "Checking Holdout Sets...Creating Holdout Sets...\n",
      "Done\n",
      "Training Model...Done\n"
     ]
    }
   ],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "joint_shap25, joint_shap_cor25, joint_train_metrics25, joint_test_metrics25 = repeated_kfold_cv(joint_model25)\n",
    "joint_shap_means25, joint_shap_cor_means25, joint_train_means25, joint_test_means25 = get_repeated_kfold_cv_summary(joint_shap25, joint_shap_cor25, joint_train_metrics25, joint_test_metrics25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Model Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([joint_shap_means25, joint_shap_means50, joint_shap_means75, joint_shap_means], keys = time_points, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_COLORS = 47630*len(merged_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color dictionary\n",
    "def generate_color(feature_name):\n",
    "    hashed = hash(feature_name) % NUM_COLORS\n",
    "    hue = hashed / 1000.0\n",
    "    saturation = (hashed % NUM_COLORS) / NUM_COLORS  # Vary the saturation within a range (0.7 to 1.0)\n",
    "    value = 0.8\n",
    "    color_rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "    return color_rgb\n",
    "\n",
    "# Create a color mapping dictionary\n",
    "color_mapping = {feature_name: generate_color(feature_name) for feature_name in merged_df.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_important_features_over_time(merged_df, color_mapping, \"Standardizing Features Across All Tasks\", \"./figures/across_task_std_btwn_features.svg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Model (standardizing features _within_ tasks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize_within defaults to false, so let's check what happens when we change it to true\n",
    "joint_model_std_within = ModelBuilder(dataset_names = [\"juries\", \"csop\", \"csopII\", \"becker\", \"dat\"], standardize_within= True)\n",
    "joint_model_std_within.select_target(target=[\"majority_pct\", \"zscore_efficiency\", \"efficiency\", \"mean_post_discussion_error_pct\", \"score\"])\n",
    "joint_model_std_within.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "joint_sw_shap, joint_sw_shap_cor, joint_sw_train_metrics, joint_sw_test_metrics = repeated_kfold_cv(joint_model_std_within)\n",
    "joint_sw_shap_means, joint_sw_shap_cor_means, joint_sw_train_means, joint_sw_test_means = get_repeated_kfold_cv_summary(joint_sw_shap, joint_sw_shap_cor, joint_sw_train_metrics, joint_sw_test_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75% of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model_std_within75 = ModelBuilder(dataset_names = [\"juries\", \"csop\", \"csopII\", \"becker\", \"dat\"], standardize_within= True, output_dir = '../output/first_75/')\n",
    "joint_model_std_within75.select_target(target=[\"majority_pct\", \"zscore_efficiency\", \"efficiency\", \"mean_post_discussion_error_pct\", \"score\"])\n",
    "joint_model_std_within75.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "joint_sw_shap75, joint_sw_shap_cor75, joint_sw_train_metrics75, joint_sw_test_metrics75 = repeated_kfold_cv(joint_model_std_within75)\n",
    "joint_sw_shap_means75, joint_sw_shap_cor_means75, joint_sw_train_means75, joint_sw_test_means75 = get_repeated_kfold_cv_summary(joint_sw_shap75, joint_sw_shap_cor75, joint_sw_train_metrics75, joint_sw_test_metrics75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50% of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model_std_within50 = ModelBuilder(dataset_names = [\"juries\", \"csop\", \"csopII\", \"becker\", \"dat\"], standardize_within= True, output_dir = '../output/first_50/')\n",
    "joint_model_std_within50.select_target(target=[\"majority_pct\", \"zscore_efficiency\", \"efficiency\", \"mean_post_discussion_error_pct\", \"score\"])\n",
    "joint_model_std_within50.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "joint_sw_shap50, joint_sw_shap_cor50, joint_sw_train_metrics50, joint_sw_test_metrics50 = repeated_kfold_cv(joint_model_std_within50)\n",
    "joint_sw_shap_means50, joint_sw_shap_cor_means50, joint_sw_train_means50, joint_sw_test_means50 = get_repeated_kfold_cv_summary(joint_sw_shap50, joint_sw_shap_cor50, joint_sw_train_metrics50, joint_sw_test_metrics50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25% of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model_std_within25 = ModelBuilder(dataset_names = [\"juries\", \"csop\", \"csopII\", \"becker\", \"dat\"], standardize_within= True, output_dir = '../output/first_75/')\n",
    "joint_model_std_within25.select_target(target=[\"majority_pct\", \"zscore_efficiency\", \"efficiency\", \"mean_post_discussion_error_pct\", \"score\"])\n",
    "joint_model_std_within25.define_model(model_type = 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Repeated k-Fold CV\n",
    "joint_sw_shap25, joint_sw_shap_cor25, joint_sw_train_metrics25, joint_sw_test_metrics25 = repeated_kfold_cv(joint_model_std_within25)\n",
    "joint_sw_shap_means25, joint_sw_shap_cor_means25, joint_sw_train_means25, joint_sw_test_means25 = get_repeated_kfold_cv_summary(joint_sw_shap25, joint_sw_shap_cor25, joint_sw_train_metrics25, joint_sw_test_metrics25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Top Feature Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([joint_sw_shap_means25, joint_sw_shap_means50, joint_sw_shap_means75, joint_sw_shap_means], keys = time_points, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_important_features_over_time(merged_df, color_mapping, \"Standardizing Features Within Each Task\", \"./figures/across_task_std_within_features.svg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary (Across and Within; Train and Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_btwn_metrics_train = pd.concat([joint_train_means25, joint_train_means50, joint_train_means75, joint_train_means], keys = time_points, axis=1) \n",
    "joint_within_metrics_train = pd.concat([joint_sw_train_means25, joint_sw_train_means50, joint_sw_train_means75, joint_sw_train_means], keys = time_points, axis=1)\n",
    "joint_btwn_metrics_test = pd.concat([joint_test_means25, joint_test_means50, joint_test_means75, joint_test_means], keys = time_points, axis=1) \n",
    "joint_within_metrics_test = pd.concat([joint_sw_test_means25, joint_sw_test_means50, joint_sw_test_means75, joint_sw_test_means], keys = time_points, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_without_broken_axis(datasets, dataset_names, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    plt.style.use({\"figure.facecolor\": \"white\", \"axes.facecolor\": \"white\"})\n",
    "\n",
    "    # Define line styles and colors from Dark2 colormap\n",
    "    num_colors = len(dataset_names)\n",
    "    colors = mpl.cm.Set2(np.linspace(0, 1, num_colors))\n",
    "\n",
    "    # Plot datasets on the same y-axis\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        dataset_name = dataset_names[i]\n",
    "        transposed_data = dataset.T\n",
    "\n",
    "        # Plot R^2 values for each dataset\n",
    "        ax.plot(time_points, transposed_data[\"r2\"], marker='o', label=dataset_name, color=colors[i], linewidth=3)\n",
    "        for x, y in zip(time_points, transposed_data[\"r2\"]):\n",
    "            ax.annotate(f'{y:.2f}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12)\n",
    "\n",
    "    # Adjust layout and labels\n",
    "    ax.tick_params(axis=\"both\", labelsize=16)  # Increase tick label size\n",
    "    ax.set_ylabel(\"R^2\", fontsize=16)\n",
    "    ax.set_xlabel(\"Percent of Chat Messages (Chronological)\", fontsize=16)\n",
    "\n",
    "    # Add legend to the right\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(lines, labels, loc=\"upper left\", fontsize=14, bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Set y-axis range with a buffer\n",
    "    ax.set_ylim(0, max(ax.get_ylim()) * 1.1)\n",
    "\n",
    "    # Adjust title position\n",
    "    plt.title(title, fontweight=\"bold\", fontsize=18)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figures/all_task_r2_graph.svg', dpi=1200, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [joint_btwn_metrics_train, joint_within_metrics_train, joint_btwn_metrics_test, joint_within_metrics_test]\n",
    "dataset_names = [\"Standardized Across-Task: Train\", \"Standardized Within-Task: Train\", \"Standardized Across-Task: Validation\", \"Standardized Within-Task: Validation\"]\n",
    "\n",
    "plot_r2_without_broken_axis(datasets, dataset_names, \"R^2 of All-Task Models Over Time\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team_process_map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4308115ec36d55d4bd05e5164490d17bc30a5f7275b0a0d4f3922ff237a9eaea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
