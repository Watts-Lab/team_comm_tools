{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agshruti/Downloads/team-process-map/.venv/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'feature_engine/features/lexicons_dict.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      7\u001b[0m lexicon_pkl_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_engine/features/lexicons_dict.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlexicon_pkl_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m lexicons_pickle_file:\n\u001b[1;32m      9\u001b[0m \tlexicons_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(lexicons_pickle_file)\n",
      "File \u001b[0;32m~/Downloads/team-process-map/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'feature_engine/features/lexicons_dict.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "lexicon_pkl_file_path = \"feature_engine/features/lexicons_dict.pkl\"\n",
    "with open(lexicon_pkl_file_path, \"rb\") as lexicons_pickle_file:\n",
    "\tlexicons_dict = pickle.load(lexicons_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'am', 'are', 'arent', 'be', 'been', 'being', 'can', 'cant', 'cannot', 'could', 'couldnt', 'did', 'didnt', 'do', 'dont', 'get', 'got', 'gotta', 'had', 'hadnt', 'hasnt', 'have', 'havent', 'is', 'isnt', 'may', 'should', 'shouldve', 'shouldnt', 'was', 'were', 'will', 'wont', 'would', 'wouldve', 'wouldnt', 'although', 'and', 'as', 'because', 'cause', 'but', 'if', 'or', 'so', 'then', 'unless', 'whereas', 'while', 'a', 'an', 'each', 'every', 'all', 'lot', 'lots', 'the', 'this', 'those', 'anybody', 'anything', 'anywhere', 'everybodys', 'everyone', 'everything', 'everythings', 'everywhere', 'he', 'hed', 'hes', 'her', 'him', 'himself', 'herself', 'his', 'I', 'Id', 'Ill', 'Im', 'Ive', 'it', 'itd', 'itll', 'its', 'its', 'itself', 'me', 'my', 'mine', 'myself', 'nobody', 'nothing', 'nowhere', 'one', 'ones', 'ones', 'our', 'ours', 'she', 'shell', 'shes', 'shed', 'somebody', 'someone', 'someplace', 'that', 'thatd', 'thatll', 'thats', 'them', 'themselves', 'these', 'they', 'theyd', 'theyll', 'theyre', 'theyve', 'us', 'we', 'wed', 'well', 'were', 'weve', 'what', 'whatd', 'whats', 'whatever', 'when', 'where', 'whered', 'wheres', 'wherever', 'which', 'who', 'whos', 'whom', 'whose', 'why', 'you', 'youd', 'youll', 'youre', 'youve', 'your', 'yours', 'yourself', 'about', 'after', 'against', 'at', 'before', 'by', 'down', 'for', 'from', 'in', 'into', 'near', 'of', 'off', 'on', 'out', 'over', 'than', 'to', 'until', 'up', 'with', 'without', 'ah', 'hi', 'huh', 'like', 'mmhmm', 'oh', 'okay', 'right', 'uh', 'uhhuh', 'um', 'well', 'yeah', 'yup', 'just', 'no', 'not', 'really', 'too', 'very']\n",
      "[['and huh be was itd', 'all anything anybody', 'get for which Id', 'from this then her to', 'herself before himself shes Ive'], [0.05, 0.03, 0.04, 0.05, 0.05]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "alphabet = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "filler = [\"Lorem\", \"ipsum\", \"dolor\", \"amet\", \"consectetur\", \"adipiscing\", \"sed\", \"euismod\", \"tempor\"]\n",
    "\n",
    "def generate_random_strings_from_wordlist(wordlist, num_strings, min_words=1, max_words=5):\n",
    "    max_words = min(max_words, len(wordlist))\n",
    "    strings = []\n",
    "    expected_value = []\n",
    "    for _ in range(num_strings):\n",
    "        num_words = random.randint(min_words, max_words)\n",
    "        expected_value.append(num_words/100)\n",
    "        selected_words = random.sample(wordlist, num_words)\n",
    "\n",
    "         # for strings in selected_words that end with *, append a random number of letters to that string\n",
    "        for i in range(len(selected_words)):\n",
    "            selected_words[i] = selected_words[i].strip()\n",
    "            if selected_words[i].endswith(\"*\"):\n",
    "                selected_words[i] = selected_words[i][:-1] + ''.join(random.sample(alphabet, random.randint(1, 5)))\n",
    "\n",
    "        num_fillers = random.randint(1, 3)\n",
    "        filler_words = random.sample(filler, num_fillers)\n",
    "        final_words = random.choices(selected_words) + random.choices(filler_words)\n",
    "        random.shuffle(final_words)\n",
    "        \n",
    "        strings.append(' '.join(final_words))\n",
    "    return [strings, expected_value]\n",
    "\n",
    "def write_to_csv(strings, expected_vals, wordlist_name, filename='output.csv'):\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for string, expected_val in zip(strings, expected_vals):\n",
    "            writer.writerow([0, 0, string, wordlist_name + \"_lexical_per_100\", expected_val])\n",
    "\n",
    "directory = ['feature_engine/features/lexicons/liwc_lexicons','feature_engine/features/lexicons/liwc_lexicons_small_test','feature_engine/features/lexicons/other_lexicons']\n",
    "parent = Path(os.getcwd()).parent\n",
    "gparent = Path(parent).parent\n",
    "# function_words_path = os.path.join(gparent, \"src/team_comm_tools/features/lexicons/function_words.txt\")\n",
    "\n",
    "# directory = [function_words_path]\n",
    "for d in directory:\n",
    "    for filename in os.listdir(d):\n",
    "        if filename.startswith(\".\"):\n",
    "            continue\n",
    "        with open(filename, encoding=\"mac_roman\") as lexicons:\n",
    "    \n",
    "            wordlist = lexicons.read().splitlines()\n",
    "            output = generate_random_strings_from_wordlist(wordlist, num_strings=5)\n",
    "            \n",
    "            filename = filename[:-4] if filename.endswith(\".txt\") else filename\n",
    "            write_to_csv(output[0], output[1], filename)\n",
    "\n",
    "# manually appended output.csv to test_chat_level.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/agshruti/Downloads/team-process-map/tests/ipython_notebooks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/agshruti/Downloads/team-process-map/src/features/lexicons/function_words.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module Path not found\n",
    "import os\n",
    "\n",
    "# print current directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# print parent directory\n",
    "parent = Path(os.getcwd()).parent\n",
    "gparent = Path(parent).parent\n",
    "os.path.join(gparent, \"src/features/lexicons/function_words.txt\")\n",
    "\n",
    "# get parent directory\n",
    "# parent_dir = Path(os.getcwd()).parent\n",
    "\n",
    "# # get file within parent directory\n",
    "# file_path = os.path.join(parent_dir, \"output.csv\")\n",
    "\n",
    "# # get file within grandparent directory\n",
    "# grandparent_dir = Path(parent_dir).parent\n",
    "# file_path = os.path.join(grandparent_dir, \"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate function & content word accommodation tests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
