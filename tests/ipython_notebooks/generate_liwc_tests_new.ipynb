{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from team_comm_tools.utils.check_embeddings import load_liwc_dict, sort_words\n",
    "from team_comm_tools.features.lexical_features_v2 import get_liwc_count\n",
    "from team_comm_tools import FeatureBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_to_preserve = {\"(:\", \"(;\", \"):\", \"/:\", \":(\", \":)\", \":/\", \";)\"} \n",
    "alphabet = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "filler = [\"Lorem\", \"ipsum\", \"dolor\", \"amet\", \"consectetur\", \"adipiscing\", \"sed\", \"euismod\", \"tempor\"]\n",
    "connectors = [\n",
    "    \",\",  # Comma\n",
    "    \".\",  # Period\n",
    "    \";\",  # Semicolon\n",
    "    \":\",  # Colon\n",
    "    \"-\",  # Hyphen\n",
    "    \"–\",  # En dash\n",
    "    \"—\",  # Em dash\n",
    "    \"!\",  # Exclamation mark\n",
    "    \"?\",  # Question mark\n",
    "    \"(\",  # Open parenthesis\n",
    "    \")\",  # Close parenthesis\n",
    "    \"[\",  # Open bracket\n",
    "    \"]\",  # Close bracket\n",
    "    \"{\",  # Open brace\n",
    "    \"}\",  # Close brace\n",
    "    \"\\\"\",  # Double quotation mark\n",
    "    \"'\",  # Single quotation mark (apostrophe)\n",
    "    \"...\",  # Ellipsis\n",
    "    \"/\",  # Slash\n",
    "    \"\\\\\",  # Backslash\n",
    "    \"|\",  # Vertical bar (pipe)\n",
    "    \"+\",  # Plus sign\n",
    "    \"=\",  # Equal sign\n",
    "    \"<\",  # Less than\n",
    "    \">\",  # Greater than\n",
    "    # \"*\",  # Asterisk\n",
    "    \"^\",  # Caret\n",
    "    \"~\",  # Tilde\n",
    "    \"$\",  # Dollar sign\n",
    "    \"€\",  # Euro sign\n",
    "    \"¥\",  # Yen sign\n",
    "    \"£\",  # Pound sign\n",
    "    \"#\",  # Hash/Pound\n",
    "    \"&\",  # Ampersand\n",
    "    \"%\",  # Percent\n",
    "    # \"_\",  # Underscore: doesn't work with word boundaries \\b\n",
    "]\n",
    "\n",
    "def fill_wordlist(selected_words: list):\n",
    "    result = []\n",
    "    for word in selected_words:\n",
    "        word = word.strip()\n",
    "        # for strings in selected_words that end with *, append a random number of letters to that string\n",
    "        if word.endswith(\"*\"):\n",
    "            word = word[:-1] #+ ''.join(random.sample(alphabet, random.randint(1, 5)))\n",
    "        result.append(word)\n",
    "        # randomly add filler words\n",
    "        num_fillers = random.randint(1, 2)\n",
    "        fillers = random.choices(filler, k=num_fillers)\n",
    "        result.extend(fillers)\n",
    "    # randomly add connectors\n",
    "    output = \"\"\n",
    "    for i, word in enumerate(result):\n",
    "        output += word\n",
    "        if i < len(result) - 1:  # Only add a connector if not the last word\n",
    "            output += random.choice(connectors)\n",
    "            output += \" \"\n",
    "\n",
    "    return output\n",
    "\n",
    "def extract_words_from_regex(pattern):\n",
    "    words = []\n",
    "    # Remove word boundaries (\\b), negative lookbehind ((?<!\\w)), and lookahead ((?!\\w))\n",
    "    cleaned_pattern = re.sub(r\"\\\\b|\\(\\?<!\\\\w\\)|\\(\\?!\\\\w\\)\", \"\", pattern)\n",
    "\n",
    "    # Split by '|' (OR operator)\n",
    "    for segment in re.split(r\"\\|\", cleaned_pattern):\n",
    "        segment = segment.strip()\n",
    "        segment = segment.replace(r\"\\(\", \"(\").replace(r\"\\)\", \")\")\n",
    "        segment_with_asterisk = re.sub(r\"\\\\S\\*|\\.\\*\", \"*\", segment)\n",
    "        words.append(segment_with_asterisk)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LIWC dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '2007' \n",
    "# version = '2015'\n",
    "\n",
    "root_dir = Path().resolve().parent.parent\n",
    "if version == '2015':\n",
    "    custom_liwc_dictionary_path = root_dir / 'src/team_comm_tools/features/lexicons/liwc_2015.dic'\n",
    "    with open(custom_liwc_dictionary_path, 'r', encoding='utf-8-sig') as file:\n",
    "        dicText = file.read()\n",
    "        lexicons_dict = load_liwc_dict(dicText)\n",
    "elif version == '2007':\n",
    "    lexicon_pkl_file_path = root_dir / \"src/team_comm_tools/features/assets/lexicons_dict.pkl\"\n",
    "    with open(lexicon_pkl_file_path, \"rb\") as lexicons_pickle_file:\n",
    "        lexicons_dict = pickle.load(lexicons_pickle_file)\n",
    "    lexicons_dict\n",
    "else:\n",
    "    raise ValueError(\"Invalid version. Please choose either 2007 or 2015.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### regenerate LIWC lexicons pkl file\n",
    "\n",
    "# def read_in_lexicons(directory, lexicons_dict):\n",
    "#     for file in directory.iterdir():\n",
    "#         if file.is_file() and not file.name.startswith(\".\"):\n",
    "#             with open(file, encoding = \"mac_roman\") as lexicons:\n",
    "#                 clean_name = re.sub('.txt', '', file.name)\n",
    "#                 lexicons_dict[clean_name] = sort_words(lexicons)\n",
    "\n",
    "# lexicons_dict = {}\n",
    "# read_in_lexicons(root_dir / \"src/team_comm_tools/features/lexicons/liwc_lexicons/\", lexicons_dict) # Reads in LIWC Lexicons\n",
    "# read_in_lexicons(root_dir / \"src/team_comm_tools/features/lexicons/other_lexicons/\", lexicons_dict) # Reads in Other Lexicons\n",
    "\n",
    "# with open(\"lexicons_dict.pkl\", \"wb\") as lexicons_pickle_file:\n",
    "#           pickle.dump(lexicons_dict, lexicons_pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One category at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!                                                                                   \n"
     ]
    }
   ],
   "source": [
    "min_words, max_words = 3, 30\n",
    "num_tests = 10\n",
    "one_cat_test_lst = []\n",
    "for i in range(num_tests):\n",
    "    error = False\n",
    "    for cat, regex in lexicons_dict.items():\n",
    "        wordList = extract_words_from_regex(regex)\n",
    "        expected_value = random.randint(min_words, min(max_words, len(wordList)))\n",
    "        selected_words = random.sample(wordList, expected_value)\n",
    "        test_string = fill_wordlist(selected_words)\n",
    "        count = get_liwc_count(lexicons_dict[cat], test_string)\n",
    "        if count != expected_value:\n",
    "            print(' ' * 100, end=\"\\r\")\n",
    "            print(f\"{cat} ERROR\")\n",
    "            print(f\"{test_string}\")\n",
    "            print(f\"expected_value: {expected_value}, count: {count}\")\n",
    "            error = True\n",
    "            break\n",
    "        else:\n",
    "            print(' ' * 100, end=\"\\r\")\n",
    "            print(f\"{cat} SUCCESS\", end=\"\\r\", flush=True)\n",
    "        one_cat_test_lst.append({\n",
    "            \"conversation_num\": f\"{i+1}_{cat}\",\n",
    "            \"speaker_nickname\": cat,\n",
    "            \"message\": test_string,\n",
    "            \"expected_column\": f\"{cat}_lexical_wordcount\",\n",
    "            \"expected_value\": expected_value\n",
    "        })\n",
    "    if error:\n",
    "        break\n",
    "else:\n",
    "    print(' ' * 100, end=\"\\r\")\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "one_cat_test_df = pd.DataFrame(one_cat_test_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mixed category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!                                                                                   \n"
     ]
    }
   ],
   "source": [
    "min_words, max_words = 3, 30\n",
    "num_tests = 10\n",
    "mix_cat_test_lst = []\n",
    "for i in range(num_tests):\n",
    "    wordList = []\n",
    "    for category, pattern in lexicons_dict.items():\n",
    "        wordList.extend(extract_words_from_regex(pattern))\n",
    "    expected_value = random.randint(min_words, min(max_words, len(wordList)))\n",
    "    selected_words = random.sample(wordList, expected_value)\n",
    "    # ground truth\n",
    "    selected_words_truth = []\n",
    "    for word in selected_words:\n",
    "        word = word.strip()\n",
    "        if word.endswith(\"*\"):\n",
    "            word = word[:-1]\n",
    "        selected_words_truth.append(word)\n",
    "    test_string_truth = \" \".join(selected_words_truth)\n",
    "    results = {category: re.findall(pattern, test_string_truth) for category, pattern in lexicons_dict.items()}\n",
    "    \n",
    "    # test string: add filler and connectors\n",
    "    test_string = fill_wordlist(selected_words)\n",
    "    test_results = {category: re.findall(pattern, test_string) for category, pattern in lexicons_dict.items()}\n",
    "    # Compare results\n",
    "    error = False\n",
    "    for cat, matches in results.items():\n",
    "        expected, found = len(matches), len(test_results[cat])\n",
    "        if expected != found:\n",
    "            print(f\"{cat} ERROR\")\n",
    "            print(f\"{test_string}\")\n",
    "            print(f\"expected_value: {expected}, count: {found}\")\n",
    "            error = True\n",
    "            break\n",
    "        else:\n",
    "            print(' ' * 100, end=\"\\r\")\n",
    "            print(f\"{cat} SUCCESS\", end=\"\\r\", flush=True)\n",
    "        mix_cat_test_lst.append({\n",
    "            \"conversation_num\": f\"{i+1}_mix\",\n",
    "            \"speaker_nickname\": cat,\n",
    "            \"message\": test_string,\n",
    "            \"expected_column\": f\"{cat}_lexical_wordcount\",\n",
    "            \"expected_value\": expected\n",
    "        })\n",
    "    if error:\n",
    "        break\n",
    "else:\n",
    "    print(' ' * 100, end=\"\\r\")\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "mix_cat_test_df = pd.DataFrame(mix_cat_test_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append to current test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_test_df = pd.concat([one_cat_test_df, mix_cat_test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chat_path = root_dir / 'tests/data/cleaned_data/test_chat_level.csv'\n",
    "test_chat_df = pd.read_csv(test_chat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chat_df = test_chat_df[~test_chat_df['expected_column'].str.contains('_lexical_wordcount')]\n",
    "test_chat_df = pd.concat([test_chat_df, liwc_test_df], ignore_index=True)\n",
    "# test_chat_df.to_csv(test_chat_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
